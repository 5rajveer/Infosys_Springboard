{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Section 1: Install Required Libraries",
   "metadata": {
    "id": "xiSvEOCBZp_h"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This cell installs all the necessary Python packages",
   "metadata": {
    "id": "cN8fOodeZy8f"
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install -q -U transformers accelerate bitsandbytes radon ipywidgets matplotlib\n\nprint(\"✅ All required libraries have been installed successfully!\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQTK7JYNZeXd",
    "outputId": "58a84251-67ad-4f97-8587-c544185f0143"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h✅ All required libraries have been installed successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "# Section 2: Imports and Configuration",
   "metadata": {
    "id": "TfzJyZg0Z2ai"
   }
  },
  {
   "cell_type": "code",
   "source": "import os\nimport re\nimport warnings\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom radon.visitors import ComplexityVisitor\nfrom radon.metrics import mi_visit\nfrom radon.raw import analyze\n\n# Suppress warnings for a cleaner output\nwarnings.filterwarnings(\"ignore\")\n\n# --- Hugging Face Token Setup ---\n# Tries to get the token from an environment variable.\n# For Google Colab, you can add your HF token to the \"Secrets\" tab (🔑 icon on the left).\n# Name the secret 'HF_TOKEN'.\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"✅ Hugging Face token loaded from Colab Secrets.\")\nexcept (ImportError, KeyError):\n    HF_TOKEN = os.getenv('HF_TOKEN')\n    if HF_TOKEN:\n        print(\"✅ Hugging Face token loaded successfully from environment variable.\")\n    else:\n        print(\"🚨 Could not load HF_TOKEN. Please add it to Colab Secrets or set it as an environment variable.\")\n\n# --- Model Definitions ---\n# A dictionary mapping a user-friendly name to its Hugging Face model ID.\nMODELS = {\n    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n    \"Stable-Code-3B\": \"stabilityai/stable-code-3b\",\n    \"Gemma-2B-IT\": \"google/gemma-2b-it\"\n}\n\n# --- Caching for Loaded Models ---\n# This dictionary will store loaded models and tokenizers to avoid reloading them.\nloaded_models_cache = {}\n\nprint(\"✅ Imports and configurations are complete.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MeIs2PcJZ1i4",
    "outputId": "f29570f6-d401-4a3d-c944-33ba598fefaf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Hugging Face token loaded from Colab Secrets.\n",
      "✅ Imports and configurations are complete.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "# Section 3: Core Functions",
   "metadata": {
    "id": "9Qo1S8IDaCd9"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This cell contains the main logic for loading models, generating code, and evaluating its quality. No changes are needed here.",
   "metadata": {
    "id": "qTbHRX6faJPu"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- Function to Load Model and Tokenizer ---\ndef load_model_and_tokenizer(model_name):\n    \"\"\"\n    Loads a model and tokenizer from Hugging Face with caching.\n    Uses bfloat16 for efficiency and device_map='auto' for GPU utilization.\n    \"\"\"\n    model_id = MODELS[model_name]\n    if model_id in loaded_models_cache:\n        print(f\"🧠 Loading {model_name} from cache...\")\n        return loaded_models_cache[model_id]\n\n    print(f\"Downloading and loading {model_name}...\")\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)\n        # For Phi-2, pad_token is not set by default. We set it to eos_token.\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            token=HF_TOKEN,\n            trust_remote_code=True\n        )\n        loaded_models_cache[model_id] = (model, tokenizer)\n        print(f\"✅ {model_name} loaded successfully.\")\n        return model, tokenizer\n    except Exception as e:\n        print(f\"❌ Error loading {model_name}: {e}\")\n        return None, None\n\n# --- Function to Generate Code ---\ndef generate_code(model, tokenizer, prompt):\n    \"\"\"\n    Generates code from a given prompt using the specified model and tokenizer.\n    \"\"\"\n    print(f\"Generating code for prompt: '{prompt[:30]}...'\")\n    # Gemma and DeepSeek models use a specific chat template for better results.\n    if any(model_type in tokenizer.name_or_path for model_type in [\"deepseek\", \"gemma\"]):\n         messages = [{\"role\": \"user\", \"content\": prompt}]\n         input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    else:\n        input_text = prompt\n\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=512,\n        do_sample=True,\n        temperature=0.2,\n        top_p=0.95,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Use regex to find code within markdown-style code blocks\n    code_match = re.search(r\"```python\\n(.*?)\\n```\", generated_text, re.DOTALL)\n    if code_match:\n        return code_match.group(1).strip()\n\n    # A more general regex for any code block\n    code_match = re.search(r\"```(.*?)```\", generated_text, re.DOTALL)\n    if code_match:\n        # Strip potential language hints like 'python'\n        return code_match.group(1).strip().lstrip('python\\\\n')\n\n    # Fallback if no code block is found: return the text after the input\n    return generated_text[len(input_text):].strip()\n\n# --- Function to Evaluate Code Quality ---\ndef evaluate_code(code_string):\n    \"\"\"\n    Analyzes a string of Python code using 'radon' and returns quality metrics.\n    \"\"\"\n    if not code_string:\n        return {\"complexity\": 0, \"mi_score\": 0, \"loc\": 0}\n    try:\n        # Use ComplexityVisitor to find functions and their complexity\n        visitor = ComplexityVisitor.from_code(code_string)\n        total_complexity = sum(f.complexity for f in visitor.functions)\n\n        # Get maintainability index and lines of code\n        mi_score = mi_visit(code_string, multi=True)\n        raw_analysis = analyze(code_string)\n        loc = raw_analysis.lloc # Logical Lines of Code\n\n        return {\n            \"complexity\": total_complexity,\n            \"mi_score\": round(mi_score, 2),\n            \"loc\": loc\n        }\n    except Exception as e:\n        # Handle cases where radon fails to parse the generated code\n        print(f\"⚠️ Radon analysis failed: {e}\")\n        return {\"complexity\": -1, \"mi_score\": -1, \"loc\": -1}\n\nprint(\"✅ Core functions defined successfully.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbaDuvtvaHXM",
    "outputId": "3363d5de-2077-4da9-8843-a7a35fa17749"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Core functions defined successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "# Section 4: Visualization Function",
   "metadata": {
    "id": "0s2QeHakaPPJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This cell defines the function for plotting the comparison charts. No changes are needed.",
   "metadata": {
    "id": "SoZt1HKBaYHs"
   }
  },
  {
   "cell_type": "code",
   "source": "# This function creates bar charts to compare model performance.\n\ndef plot_metrics(results):\n    \"\"\"\n    Generates and displays three bar plots for the collected metrics.\n    \"\"\"\n    model_names = list(results.keys())\n    # Filter out any models where analysis failed\n    model_names = [name for name in model_names if results[name]['metrics']['complexity'] != -1]\n    if not model_names:\n        print(\"No valid results to plot.\")\n        return\n\n    complexity_scores = [results[name]['metrics']['complexity'] for name in model_names]\n    mi_scores = [results[name]['metrics']['mi_score'] for name in model_names]\n    loc_scores = [results[name]['metrics']['loc'] for name in model_names]\n\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n    fig.suptitle('Code Generation Model Performance Comparison', fontsize=16)\n    colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n\n    # Plot 1: Cyclomatic Complexity\n    ax1.bar(model_names, complexity_scores, color=colors)\n    ax1.set_title('Cyclomatic Complexity (Lower is Better)')\n    ax1.set_ylabel('Complexity Score')\n    ax1.tick_params(axis='x', rotation=45)\n\n    # Plot 2: Maintainability Index\n    ax2.bar(model_names, mi_scores, color=colors)\n    ax2.set_title('Maintainability Index (Higher is Better)')\n    ax2.set_ylabel('MI Score')\n    ax2.tick_params(axis='x', rotation=45)\n    ax2.set_ylim(0, 100) # MI score is typically 0-100\n\n    # Plot 3: Logical Lines of Code\n    ax3.bar(model_names, loc_scores, color=colors)\n    ax3.set_title('Logical Lines of Code (LOC)')\n    ax3.set_ylabel('Number of Lines')\n    ax3.tick_params(axis='x', rotation=45)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()\n\nprint(\"✅ Visualization function defined successfully.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qumrK-QHaVa5",
    "outputId": "d4b59e14-15e3-4ebc-cb8f-392dea07f95f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Visualization function defined successfully.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "# Section 5: Sample Prompts",
   "metadata": {
    "id": "AFSzGaQWabJ2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This cell contains the list of sample prompts for the UIs. No changes are needed.",
   "metadata": {
    "id": "mEUieuyyag_0"
   }
  },
  {
   "cell_type": "code",
   "source": "# A list of diverse prompts to test the models.\n\nSAMPLE_PROMPTS = [\n    # Python / Data Science\n    \"Write a Python function that takes a pandas DataFrame and returns the correlation matrix for its numeric columns.\",\n    \"Generate a Python script to scrape the headlines from the BBC News homepage using BeautifulSoup.\",\n    \"Create a Python function to implement the bubble sort algorithm.\",\n    # Web Development\n    \"Write a simple HTML page with a form that has fields for 'name', 'email' and a 'submit' button.\",\n    \"Generate a JavaScript function that fetches data from an API endpoint and logs the JSON response to the console.\",\n    # Database\n    \"Write a SQL query to find all employees who earn more than the average salary of their respective departments.\",\n    \"Write a SQL query to select the top 5 most sold products from a 'sales' table.\",\n    # General Purpose / DevOps\n    \"Create a Python function that lists all files in a directory and its subdirectories.\",\n    \"Write a simple Dockerfile for a basic Python Flask application.\",\n    # Algorithmic\n    \"Write a Python function to find the factorial of a number using recursion.\"\n]\n\nprint(f\"✅ {len(SAMPLE_PROMPTS)} sample prompts are ready.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpFTD6d1agSO",
    "outputId": "f3a8dcca-70ee-418f-dbc6-87ba643d48b5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ 10 sample prompts are ready.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "# Section 6: Interactive UI #1 (Benchmark All Models)",
   "metadata": {
    "id": "OcMMHZzraoPB"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This UI will now benchmark all models except for the Replit one. No code changes were needed here, as it automatically reads from the updated MODELS dictionary.",
   "metadata": {
    "id": "HdcGiR1Caxzx"
   }
  },
  {
   "cell_type": "code",
   "source": "# This UI benchmarks all models defined in the MODELS dictionary for a given prompt.\n\n# --- UI Widgets ---\nprompt_input_1 = widgets.Dropdown(\n    options=SAMPLE_PROMPTS,\n    description='Prompt:',\n    style={'description_width': 'initial'},\n    layout={'width': '95%'}\n)\ncustom_prompt_input_1 = widgets.Textarea(\n    placeholder='Or type your own custom prompt here...',\n    layout={'width': '95%', 'height': '80px'}\n)\nbenchmark_button = widgets.Button(description=\"Benchmark All Models\", button_style='success')\noutput_1 = widgets.Output()\n\n# --- Event Handler ---\ndef on_benchmark_button_clicked(b):\n    with output_1:\n        clear_output(wait=True)\n        # Use the custom prompt if filled, otherwise use the dropdown value\n        prompt = custom_prompt_input_1.value if custom_prompt_input_1.value else prompt_input_1.value\n        if not prompt:\n            print(\"🚨 Please select or enter a prompt.\")\n            return\n\n        print(f\"🚀 Starting benchmark for prompt: '{prompt}'\\n\" + \"=\"*52)\n        results = {}\n        for model_name in MODELS.keys():\n            print(f\"--- Processing Model: {model_name} ---\")\n            model, tokenizer = load_model_and_tokenizer(model_name)\n            if model is None: continue\n\n            generated_code = generate_code(model, tokenizer, prompt)\n            metrics = evaluate_code(generated_code)\n            results[model_name] = {\"code\": generated_code, \"metrics\": metrics}\n\n            print(f\"\\n✨ Generated Code:\\n```python\\n{generated_code}\\n```\")\n            print(f\"\\n📊 Metrics: {metrics}\")\n            print(\"\\n\" + \"-\"*52 + \"\\n\")\n\n        print(\"📈 Generating performance plots...\")\n        plot_metrics(results)\n\nbenchmark_button.on_click(on_benchmark_button_clicked)\n\n# --- Display UI ---\nprint(\"UI #1 is ready. Choose a prompt and click the button to start.\")\ndisplay(prompt_input_1, custom_prompt_input_1, benchmark_button, output_1)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZLwIQ6IPatVP",
    "outputId": "3228fd05-537d-4b0e-c028-ffe2e40c4f50"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UI #1 is ready. Choose a prompt and click the button to start.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Dropdown(description='Prompt:', layout=Layout(width='95%'), options=('Write a Python function that takes a pan…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b2ce859d3074219bf0a1df03bcd951a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Textarea(value='', layout=Layout(height='80px', width='95%'), placeholder='Or type your own custom prompt here…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e93e86be8ed4e85bd5b720ffa7892a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Button(button_style='success', description='Benchmark All Models', style=ButtonStyle())"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63cde348f60c4990aafe2ad7458fdc42"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82278de920b046c79803b2829959a56b"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "# Section 7: Interactive UI #2 (Select Models to Inspect)",
   "metadata": {
    "id": "lvUUhrNTbg3D"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This UI will now show checkboxes for the updated list of models. No code changes were needed here either.",
   "metadata": {
    "id": "GRe4p4FXblJd"
   }
  },
  {
   "cell_type": "code",
   "source": "# This UI allows you to select specific models to compare using checkboxes.\n\n# --- UI Widgets ---\nprompt_input_2 = widgets.Dropdown(\n    options=SAMPLE_PROMPTS,\n    description='Prompt:',\n    style={'description_width': 'initial'},\n    layout={'width': '95%'}\n)\ncustom_prompt_input_2 = widgets.Textarea(\n    placeholder='Or type your own custom prompt here...',\n    layout={'width': '95%', 'height': '80px'}\n)\n# Dynamically create checkboxes based on the MODELS dictionary\nmodel_checkboxes = [widgets.Checkbox(value=True, description=name) for name in MODELS.keys()]\ncheckbox_container = widgets.HBox(model_checkboxes)\ninspect_button = widgets.Button(description=\"Generate & Inspect Selected\", button_style='primary')\noutput_2 = widgets.Output()\n\n# --- Event Handler ---\ndef on_inspect_button_clicked(b):\n    with output_2:\n        clear_output(wait=True)\n        prompt = custom_prompt_input_2.value if custom_prompt_input_2.value else prompt_input_2.value\n        if not prompt:\n            print(\"🚨 Please select or enter a prompt.\")\n            return\n\n        selected_models = [cb.description for cb in model_checkboxes if cb.value]\n        if not selected_models:\n            print(\"🚨 Please select at least one model to inspect.\")\n            return\n\n        print(f\"🚀 Starting inspection for prompt: '{prompt}'\")\n        print(f\"Models selected: {', '.join(selected_models)}\\n\" + \"=\"*52)\n        results = {}\n        for model_name in selected_models:\n            print(f\"--- Processing Model: {model_name} ---\")\n            model, tokenizer = load_model_and_tokenizer(model_name)\n            if model is None: continue\n\n            generated_code = generate_code(model, tokenizer, prompt)\n            metrics = evaluate_code(generated_code)\n            results[model_name] = {\"code\": generated_code, \"metrics\": metrics}\n\n            print(f\"\\n✨ Generated Code:\\n```python\\n{generated_code}\\n```\")\n            print(f\"\\n📊 Metrics: {metrics}\")\n            print(\"\\n\" + \"-\"*52 + \"\\n\")\n\n        print(\"📈 Generating performance plots...\")\n        plot_metrics(results)\n\ninspect_button.on_click(on_inspect_button_clicked)\n\n# --- Display UI ---\nprint(\"\\n\\nUI #2 is ready. Choose a prompt, select models, and click the button.\")\ndisplay(prompt_input_2, custom_prompt_input_2, checkbox_container, inspect_button, output_2)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wfsZJxubkcl",
    "outputId": "3dbcdefe-f3a9-49d5-eed6-062e94c49091"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "🚀 Starting inspection for prompt: 'Write a Python function that takes a pandas DataFrame and returns the correlation matrix for its numeric columns.'\n",
      "Models selected: DeepSeek-Coder-1.3B, Phi-2-2.7B, Stable-Code-3B, Gemma-2B-IT\n",
      "====================================================\n",
      "--- Processing Model: DeepSeek-Coder-1.3B ---\n",
      "🧠 Loading DeepSeek-Coder-1.3B from cache...\n",
      "Generating code for prompt: 'Write a Python function that t...'\n",
      "\n",
      "✨ Generated Code:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def correlation_matrix(df):\n",
      "    return df.corr()\n",
      "```\n",
      "\n",
      "📊 Metrics: {'complexity': 1, 'mi_score': 100.0, 'loc': 3}\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Processing Model: Phi-2-2.7B ---\n",
      "🧠 Loading Phi-2-2.7B from cache...\n",
      "Generating code for prompt: 'Write a Python function that t...'\n",
      "\n",
      "✨ Generated Code:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def correlation_matrix(df):\n",
      "    return df.corr()\n",
      "\n",
      "# Example usage\n",
      "df = pd.read_csv('data.csv')\n",
      "corr_matrix = correlation_matrix(df)\n",
      "print(corr_matrix)\n",
      "```\n",
      "\n",
      "📊 Metrics: {'complexity': 1, 'mi_score': 100.0, 'loc': 6}\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Processing Model: Stable-Code-3B ---\n",
      "🧠 Loading Stable-Code-3B from cache...\n",
      "Generating code for prompt: 'Write a Python function that t...'\n",
      "⚠️ Radon analysis failed: unexpected indent (<unknown>, line 9)\n",
      "\n",
      "✨ Generated Code:\n",
      "```python\n",
      "\"\"\"\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def corr_matrix(df):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    df : pandas DataFrame\n",
      "        DataFrame to compute the correlation matrix for.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    corr_matrix : pandas DataFrame\n",
      "        Correlation matrix for the DataFrame.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Compute the correlation matrix\n",
      "    corr_matrix = df.corr()\n",
      "    \n",
      "    # Return the correlation matrix\n",
      "    return corr_matrix\n",
      "\n",
      "# Test your function\n",
      "df = pd.read_csv('data/titanic.csv')\n",
      "corr_matrix = corr_matrix(df)\n",
      "print(corr_matrix)\n",
      "\n",
      "joshuadavies/DS-Take-Home-Challenge\n",
      "\"\"\"\n",
      "\n",
      "Write a Python function that takes a pandas DataFrame and returns the correlation matrix for its numeric columns.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def corr_matrix(df):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    df : pandas DataFrame\n",
      "        DataFrame to compute the correlation matrix for.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    corr_matrix : pandas DataFrame\n",
      "        Correlation matrix for the DataFrame.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Compute the correlation matrix\n",
      "    corr_matrix = df.corr()\n",
      "    \n",
      "    # Return the correlation matrix\n",
      "    return corr_matrix\n",
      "\n",
      "# Test your function\n",
      "df = pd.read_csv('data/titanic.csv')\n",
      "corr_matrix = corr_matrix(df)\n",
      "print(corr_matrix)\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "Write a Python function that takes a pandas DataFrame and returns the correlation matrix for its numeric columns.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def corr_matrix(df):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    df : pandas DataFrame\n",
      "        DataFrame to compute the correlation matrix for.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    corr_matrix : pandas DataFrame\n",
      "        Correlation matrix for the DataFrame.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Compute the correlation matrix\n",
      "    corr_matrix = df.corr()\n",
      "```\n",
      "\n",
      "📊 Metrics: {'complexity': -1, 'mi_score': -1, 'loc': -1}\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Processing Model: Gemma-2B-IT ---\n",
      "🧠 Loading Gemma-2B-IT from cache...\n",
      "Generating code for prompt: 'Write a Python function that t...'\n",
      "\n",
      "✨ Generated Code:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def calculate_correlation_matrix(dataframe):\n",
      "  \"\"\"\n",
      "  Calculates the correlation matrix for numeric columns in a pandas DataFrame.\n",
      "\n",
      "  Args:\n",
      "    dataframe: A pandas DataFrame containing numeric columns.\n",
      "\n",
      "  Returns:\n",
      "    A pandas DataFrame containing the correlation matrix.\n",
      "  \"\"\"\n",
      "\n",
      "  # Select numeric columns\n",
      "  numeric_columns = dataframe.select_dtypes(include='float64', exclude='object')\n",
      "\n",
      "  # Calculate the correlation matrix\n",
      "  corr_matrix = dataframe.corr().corr().corr().fillna(0)\n",
      "\n",
      "  return corr_matrix\n",
      "```\n",
      "\n",
      "📊 Metrics: {'complexity': 1, 'mi_score': 100.0, 'loc': 6}\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "📈 Generating performance plots...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 2000x600 with 3 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8YAAAJRCAYAAAA3XN0MAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA3YJJREFUeJzs3Xd4FNXbxvF7E9IISagJoSYEpNcgCIQeCB2kSZEuRQQEbKAiHQQVKSJKEZCiFAVRKQKCdJGmSC+hKL2FHiA57x+8uz