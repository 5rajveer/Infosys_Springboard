{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "state": {},
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes radon ipywidgets matplotlib\n",
        "\n",
        "print(\"‚úÖ All required libraries have been installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSnpBp6H3v6f",
        "outputId": "78469698-da0f-4874-caa1-565efe348061"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All required libraries have been installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports and Global Configuration üìö"
      ],
      "metadata": {
        "id": "UVFbGjz9Bf1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. Imports and Global Configuration üìö\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from radon.visitors import ComplexityVisitor\n",
        "from radon.metrics import mi_visit\n",
        "from radon.raw import analyze\n",
        "\n",
        "# Suppress warnings for a cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Hugging Face Token Setup ---\n",
        "# Tries to get the token from an environment variable.\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "if HF_TOKEN:\n",
        "    print(\"‚úÖ Hugging Face token loaded successfully from environment variable.\")\n",
        "else:\n",
        "    print(\"üö® Could not load HF_TOKEN. Please ensure it's set as an environment variable.\")\n",
        "\n",
        "# --- Model Definitions ---\n",
        "# A dictionary mapping a user-friendly name to its Hugging Face model ID.\n",
        "MODELS = {\n",
        "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
        "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "    \"Stable-Code-3B\": \"stabilityai/stable-code-3b\",\n",
        "    \"Replit-Code-3B\": \"replit/replit-code-v1-3b\",\n",
        "}\n",
        "\n",
        "# --- Caching for Loaded Models ---\n",
        "# This dictionary will store loaded models and tokenizers to avoid reloading them.\n",
        "loaded_models_cache = {}\n",
        "\n",
        "print(\"‚úÖ Imports and configurations are complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcXiERwL35JW",
        "outputId": "e68c48fb-ecac-4a1f-a965-f02ef0f0c334"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö® Could not load HF_TOKEN. Please ensure it's set as an environment variable.\n",
            "‚úÖ Imports and configurations are complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Core Functions: Model Loading, Generation, and Evaluation üöÄ"
      ],
      "metadata": {
        "id": "OwYihRY9Bixz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. Core Functions: Model Loading, Generation, and Evaluation üöÄ\n",
        "# =============================================================================\n",
        "\n",
        "# --- Function to Load Model and Tokenizer ---\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    \"\"\"\n",
        "    Loads a model and tokenizer from Hugging Face with caching.\n",
        "    Uses bfloat16 for efficiency and device_map='auto' for GPU utilization.\n",
        "    \"\"\"\n",
        "    model_id = MODELS[model_name]\n",
        "    if model_id in loaded_models_cache:\n",
        "        print(f\"üß† Loading {model_name} from cache...\")\n",
        "        return loaded_models_cache[model_id]\n",
        "\n",
        "    print(f\"Downloading and loading {model_name}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)\n",
        "        # For Phi-2, pad_token is not set by default. We set it to eos_token.\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            token=HF_TOKEN,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        loaded_models_cache[model_id] = (model, tokenizer)\n",
        "        print(f\"‚úÖ {model_name} loaded successfully.\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# --- Function to Generate Code ---\n",
        "def generate_code(model, tokenizer, prompt):\n",
        "    \"\"\"\n",
        "    Generates code from a given prompt using the specified model and tokenizer.\n",
        "    \"\"\"\n",
        "    print(f\"Generating code for prompt: '{prompt[:30]}...'\")\n",
        "    if \"gemma\" in tokenizer.name_or_path or \"deepseek\" in tokenizer.name_or_path:\n",
        "         messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "         input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        input_text = prompt\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    code_match = re.search(r\"```python\\n(.*?)\\n```\", generated_text, re.DOTALL)\n",
        "    if code_match:\n",
        "        return code_match.group(1).strip()\n",
        "    code_match = re.search(r\"```(.*?)```\", generated_text, re.DOTALL)\n",
        "    if code_match:\n",
        "        return code_match.group(1).strip().lstrip('python\\n')\n",
        "    return generated_text[len(input_text):].strip()\n",
        "\n",
        "# --- Function to Evaluate Code Quality ---\n",
        "def evaluate_code(code_string):\n",
        "    \"\"\"\n",
        "    Analyzes a string of Python code using 'radon' and returns quality metrics.\n",
        "    \"\"\"\n",
        "    if not code_string:\n",
        "        return {\"complexity\": 0, \"mi_score\": 0, \"loc\": 0}\n",
        "    try:\n",
        "        visitor = ComplexityVisitor.from_code(code_string)\n",
        "        total_complexity = sum(f.complexity for f in visitor.functions)\n",
        "        mi_score = mi_visit(code_string, multi=True)\n",
        "        raw_analysis = analyze(code_string)\n",
        "        loc = raw_analysis.lloc\n",
        "        return {\n",
        "            \"complexity\": total_complexity,\n",
        "            \"mi_score\": round(mi_score, 2),\n",
        "            \"loc\": loc\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Radon analysis failed: {e}\")\n",
        "        return {\"complexity\": -1, \"mi_score\": -1, \"loc\": -1}\n",
        "\n",
        "print(\"‚úÖ Core functions defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHmbEXu_38Wx",
        "outputId": "3d007c5c-deaa-41a4-e95c-54e05112f712"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Core functions defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualization Function üìä"
      ],
      "metadata": {
        "id": "jAjdISLZBlb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. Visualization Function üìä\n",
        "# =============================================================================\n",
        "# This function creates bar charts to compare model performance.\n",
        "\n",
        "def plot_metrics(results):\n",
        "    \"\"\"\n",
        "    Generates and displays three bar plots for the collected metrics.\n",
        "    \"\"\"\n",
        "    model_names = list(results.keys())\n",
        "    model_names = [name for name in model_names if results[name]['metrics']['complexity'] != -1]\n",
        "    if not model_names:\n",
        "        print(\"No valid results to plot.\")\n",
        "        return\n",
        "\n",
        "    complexity_scores = [results[name]['metrics']['complexity'] for name in model_names]\n",
        "    mi_scores = [results[name]['metrics']['mi_score'] for name in model_names]\n",
        "    loc_scores = [results[name]['metrics']['loc'] for name in model_names]\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    fig.suptitle('Code Generation Model Performance Comparison', fontsize=16)\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    ax1.bar(model_names, complexity_scores, color=colors)\n",
        "    ax1.set_title('Cyclomatic Complexity (Lower is Better)')\n",
        "    ax1.set_ylabel('Complexity Score')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    ax2.bar(model_names, mi_scores, color=colors)\n",
        "    ax2.set_title('Maintainability Index (Higher is Better)')\n",
        "    ax2.set_ylabel('MI Score')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.set_ylim(0, 100)\n",
        "\n",
        "    ax3.bar(model_names, loc_scores, color=colors)\n",
        "    ax3.set_title('Logical Lines of Code (LOC)')\n",
        "    ax3.set_ylabel('Number of Lines')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization function defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwDlmp4U39o6",
        "outputId": "b21ddfc1-7336-481b-d6bf-b2cf4ec6c8b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Visualization function defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sample Prompts for Testing üß™"
      ],
      "metadata": {
        "id": "mXfHLBSZBnvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. Sample Prompts for Testing üß™\n",
        "# =============================================================================\n",
        "# A list of diverse prompts to test the models.\n",
        "\n",
        "SAMPLE_PROMPTS = [\n",
        "    # Python / Data Science\n",
        "    \"Write a Python function that takes a pandas DataFrame and returns the correlation matrix for its numeric columns.\",\n",
        "    \"Generate a Python script to scrape the headlines from the BBC News homepage using BeautifulSoup.\",\n",
        "    \"Create a Python function to implement the bubble sort algorithm.\",\n",
        "    # Web Development\n",
        "    \"Write a simple HTML page with a form that has fields for 'name', 'email' and a 'submit' button.\",\n",
        "    \"Generate a JavaScript function that fetches data from an API endpoint and logs the JSON response to the console.\",\n",
        "    # Database\n",
        "    \"Write a SQL query to find all employees who earn more than the average salary of their respective departments.\",\n",
        "    \"Write a SQL query to select the top 5 most sold products from a 'sales' table.\",\n",
        "    # General Purpose / DevOps\n",
        "    \"Create a Python function that lists all files in a directory and its subdirectories.\",\n",
        "    \"Write a simple Dockerfile for a basic Python Flask application.\",\n",
        "    # Algorithmic\n",
        "    \"Write a Python function to find the factorial of a number using recursion.\"\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ {len(SAMPLE_PROMPTS)} sample prompts are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bvc_xAy4A6t",
        "outputId": "aab84b27-b511-4571-9b94-067f84ef1758"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 10 sample prompts are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Interactive UI #1: Benchmark All Models ‚ö°Ô∏è"
      ],
      "metadata": {
        "id": "8M40ocKBBp5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. Interactive UI #1: Benchmark All Models ‚ö°Ô∏è\n",
        "# =============================================================================\n",
        "# This UI benchmarks all 5 models for a given prompt.\n",
        "\n",
        "# --- UI Widgets ---\n",
        "prompt_input_1 = widgets.Dropdown(\n",
        "    options=SAMPLE_PROMPTS,\n",
        "    description='Prompt:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout={'width': '95%'}\n",
        ")\n",
        "custom_prompt_input_1 = widgets.Textarea(\n",
        "    placeholder='Or type your own custom prompt here...',\n",
        "    layout={'width': '95%', 'height': '80px'}\n",
        ")\n",
        "benchmark_button = widgets.Button(description=\"Benchmark All Models\", button_style='success')\n",
        "output_1 = widgets.Output()\n",
        "\n",
        "# --- Event Handler ---\n",
        "def on_benchmark_button_clicked(b):\n",
        "    with output_1:\n",
        "        clear_output(wait=True)\n",
        "        prompt = custom_prompt_input_1.value if custom_prompt_input_1.value else prompt_input_1.value\n",
        "        if not prompt:\n",
        "            print(\"üö® Please select or enter a prompt.\")\n",
        "            return\n",
        "\n",
        "        print(f\"üöÄ Starting benchmark for prompt: '{prompt}'\\n\" + \"=\"*52)\n",
        "        results = {}\n",
        "        for model_name in MODELS.keys():\n",
        "            print(f\"--- Processing Model: {model_name} ---\")\n",
        "            model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "            if model is None: continue\n",
        "\n",
        "            generated_code = generate_code(model, tokenizer, prompt)\n",
        "            metrics = evaluate_code(generated_code)\n",
        "            results[model_name] = {\"code\": generated_code, \"metrics\": metrics}\n",
        "\n",
        "            print(f\"\\n‚ú® Generated Code:\\n```python\\n{generated_code}\\n```\")\n",
        "            print(f\"\\nüìä Metrics: {metrics}\")\n",
        "            print(\"\\n\" + \"-\"*52 + \"\\n\")\n",
        "\n",
        "        print(\"üìà Generating performance plots...\")\n",
        "        plot_metrics(results)\n",
        "\n",
        "benchmark_button.on_click(on_benchmark_button_clicked)\n",
        "\n",
        "# --- Display UI ---\n",
        "print(\"UI #1 is ready. Choose a prompt and click the button to start.\")\n",
        "display(prompt_input_1, custom_prompt_input_1, benchmark_button, output_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p7ibTLdb4DbD",
        "outputId": "c89df4df-5d4f-4701-acb8-e0055c9beb72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting benchmark for prompt: 'Create a Python function to implement the bubble sort algorithm.'\n",
            "====================================================\n",
            "--- Processing Model: DeepSeek-Coder-1.3B ---\n",
            "üß† Loading DeepSeek-Coder-1.3B from cache...\n",
            "Generating code for prompt: 'Create a Python function to im...'\n",
            "\n",
            "‚ú® Generated Code:\n",
            "```python\n",
            "def bubble_sort(arr):\n",
            "    n = len(arr)\n",
            "\n",
            "    for i in range(n):\n",
            "        for j in range(0, n - i - 1):\n",
            "            if arr[j] > arr[j + 1]:\n",
            "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
            "\n",
            "    return arr\n",
            "```\n",
            "\n",
            "üìä Metrics: {'complexity': 4, 'mi_score': 68.59, 'loc': 7}\n",
            "\n",
            "----------------------------------------------------\n",
            "\n",
            "--- Processing Model: Phi-2-2.7B ---\n",
            "üß† Loading Phi-2-2.7B from cache...\n",
            "Generating code for prompt: 'Create a Python function to im...'\n",
            "\n",
            "‚ú® Generated Code:\n",
            "```python\n",
            "def bubble_sort(arr):\n",
            "    n = len(arr)\n",
            "    for i in range(n):\n",
            "        for j in range(0, n-i-1):\n",
            "            if arr[j] > arr[j+1]:\n",
            "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
            "    return arr\n",
            "```\n",
            "\n",
            "üìä Metrics: {'complexity': 4, 'mi_score': 68.59, 'loc': 7}\n",
            "\n",
            "----------------------------------------------------\n",
            "\n",
            "--- Processing Model: Gemma-2B-IT ---\n",
            "Downloading and loading Gemma-2B-IT...\n",
            "‚ùå Error loading Gemma-2B-IT: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
            "401 Client Error. (Request ID: Root=1-68ea212d-14b086e12ebac1e75623e644;75fcab4d-bcda-456b-81be-6bbe5e1debae)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
            "--- Processing Model: Stable-Code-3B ---\n",
            "üß† Loading Stable-Code-3B from cache...\n",
            "Generating code for prompt: 'Create a Python function to im...'\n",
            "‚ö†Ô∏è Radon analysis failed: expected ':' (<unknown>, line 70)\n",
            "\n",
            "‚ú® Generated Code:\n",
            "```python\n",
            "def bubble_sort(arr):\n",
            "    \"\"\"\n",
            "    Bubble sort algorithm.\n",
            "    \"\"\"\n",
            "    for i in range(len(arr)):\n",
            "        for j in range(len(arr) - 1):\n",
            "            if arr[j] > arr[j + 1]:\n",
            "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
            "    return arr\n",
            "\n",
            "# Create a Python function to implement the selection sort algorithm.\n",
            "\n",
            "def selection_sort(arr):\n",
            "    \"\"\"\n",
            "    Selection sort algorithm.\n",
            "    \"\"\"\n",
            "    for i in range(len(arr)):\n",
            "        min_index = i\n",
            "        for j in range(i + 1, len(arr)):\n",
            "            if arr[j] < arr[min_index]:\n",
            "                min_index = j\n",
            "        arr[i], arr[min_index] = arr[min_index], arr[i]\n",
            "    return arr\n",
            "\n",
            "# Create a Python function to implement the insertion sort algorithm.\n",
            "\n",
            "def insertion_sort(arr):\n",
            "    \"\"\"\n",
            "    Insertion sort algorithm.\n",
            "    \"\"\"\n",
            "    for i in range(1, len(arr)):\n",
            "        key = arr[i]\n",
            "        j = i - 1\n",
            "        while j >= 0 and key < arr[j]:\n",
            "            arr[j + 1] = arr[j]\n",
            "            j -= 1\n",
            "        arr[j + 1] = key\n",
            "    return arr\n",
            "\n",
            "# Create a Python function to implement the merge sort algorithm.\n",
            "\n",
            "def merge_sort(arr):\n",
            "    \"\"\"\n",
            "    Merge sort algorithm.\n",
            "    \"\"\"\n",
            "    if len(arr) > 1:\n",
            "        mid = len(arr) // 2\n",
            "        left = arr[:mid]\n",
            "        right = arr[mid:]\n",
            "\n",
            "        merge_sort(left)\n",
            "        merge_sort(right)\n",
            "\n",
            "        i = j = k = 0\n",
            "\n",
            "        while i < len(left) and j < len(right):\n",
            "            if left[i] < right[j]:\n",
            "                arr[k] = left[i]\n",
            "                i += 1\n",
            "            else:\n",
            "                arr[k] = right[j]\n",
            "                j += 1\n",
            "            k += 1\n",
            "\n",
            "        while i < len(left):\n",
            "            arr[k] = left[i]\n",
            "            i += 1\n",
            "            k += 1\n",
            "\n",
            "        while j\n",
            "```\n",
            "\n",
            "üìä Metrics: {'complexity': -1, 'mi_score': -1, 'loc': -1}\n",
            "\n",
            "----------------------------------------------------\n",
            "\n",
            "--- Processing Model: Replit-Code-3B ---\n",
            "Downloading and loading Replit-Code-3B...\n",
            "‚ùå Error loading Replit-Code-3B: ReplitLMTokenizer has no attribute vocab_size\n",
            "üìà Generating performance plots...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x600 with 3 Axes>"
            ],
            "image/