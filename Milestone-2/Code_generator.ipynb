{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "state": {},
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes radon ipywidgets matplotlib\n",
        "\n",
        "print(\"‚úÖ All required libraries have been installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSnpBp6H3v6f",
        "outputId": "78469698-da0f-4874-caa1-565efe348061"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ All required libraries have been installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports and Global Configuration üìö"
      ],
      "metadata": {
        "id": "UVFbGjz9Bf1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. Imports and Global Configuration üìö\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from radon.visitors import ComplexityVisitor\n",
        "from radon.metrics import mi_visit\n",
        "from radon.raw import analyze\n",
        "\n",
        "# Suppress warnings for a cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Hugging Face Token Setup ---\n",
        "# Tries to get the token from an environment variable.\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "if HF_TOKEN:\n",
        "    print(\"‚úÖ Hugging Face token loaded successfully from environment variable.\")\n",
        "else:\n",
        "    print(\"üö® Could not load HF_TOKEN. Please ensure it's set as an environment variable.\")\n",
        "\n",
        "# --- Model Definitions ---\n",
        "# A dictionary mapping a user-friendly name to its Hugging Face model ID.\n",
        "MODELS = {\n",
        "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
        "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "    \"Stable-Code-3B\": \"stabilityai/stable-code-3b\",\n",
        "    \"Replit-Code-3B\": \"replit/replit-code-v1-3b\",\n",
        "}\n",
        "\n",
        "# --- Caching for Loaded Models ---\n",
        "# This dictionary will store loaded models and tokenizers to avoid reloading them.\n",
        "loaded_models_cache = {}\n",
        "\n",
        "print(\"‚úÖ Imports and configurations are complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcXiERwL35JW",
        "outputId": "e68c48fb-ecac-4a1f-a965-f02ef0f0c334"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö® Could not load HF_TOKEN. Please ensure it's set as an environment variable.\n",
            "‚úÖ Imports and configurations are complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Core Functions: Model Loading, Generation, and Evaluation üöÄ"
      ],
      "metadata": {
        "id": "OwYihRY9Bixz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. Core Functions: Model Loading, Generation, and Evaluation üöÄ\n",
        "# =============================================================================\n",
        "\n",
        "# --- Function to Load Model and Tokenizer ---\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    \"\"\"\n",
        "    Loads a model and tokenizer from Hugging Face with caching.\n",
        "    Uses bfloat16 for efficiency and device_map='auto' for GPU utilization.\n",
        "    \"\"\"\n",
        "    model_id = MODELS[model_name]\n",
        "    if model_id in loaded_models_cache:\n",
        "        print(f\"üß† Loading {model_name} from cache...\")\n",
        "        return loaded_models_cache[model_id]\n",
        "\n",
        "    print(f\"Downloading and loading {model_name}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)\n",
        "        # For Phi-2, pad_token is not set by default. We set it to eos_token.\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            token=HF_TOKEN,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        loaded_models_cache[model_id] = (model, tokenizer)\n",
        "        print(f\"‚úÖ {model_name} loaded successfully.\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# --- Function to Generate Code ---\n",
        "def generate_code(model, tokenizer, prompt):\n",
        "    \"\"\"\n",
        "    Generates code from a given prompt using the specified model and tokenizer.\n",
        "    \"\"\"\n",
        "    print(f\"Generating code for prompt: '{prompt[:30]}...'\")\n",
        "    if \"gemma\" in tokenizer.name_or_path or \"deepseek\" in tokenizer.name_or_path:\n",
        "         messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "         input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        input_text = prompt\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    code_match = re.search(r\"```python\\n(.*?)\\n```\", generated_text, re.DOTALL)\n",
        "    if code_match:\n",
        "        return code_match.group(1).strip()\n",
        "    code_match = re.search(r\"```(.*?)```\", generated_text, re.DOTALL)\n",
        "    if code_match:\n",
        "        return code_match.group(1).strip().lstrip('python\\n')\n",
        "    return generated_text[len(input_text):].strip()\n",
        "\n",
        "# --- Function to Evaluate Code Quality ---\n",
        "def evaluate_code(code_string):\n",
        "    \"\"\"\n",
        "    Analyzes a string of Python code using 'radon' and returns quality metrics.\n",
        "    \"\"\"\n",
        "    if not code_string:\n",
        "        return {\"complexity\": 0, \"mi_score\": 0, \"loc\": 0}\n",
        "    try:\n",
        "        visitor = ComplexityVisitor.from_code(code_string)\n",
        "        total_complexity = sum(f.complexity for f in visitor.functions)\n",
        "        mi_score = mi_visit(code_string, multi=True)\n",
        "        raw_analysis = analyze(code_string)\n",
        "        loc = raw_analysis.lloc\n",
        "        return {\n",
        "            \"complexity\": total_complexity,\n",
        "            \"mi_score\": round(mi_score, 2),\n",
        "            \"loc\": loc\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Radon analysis failed: {e}\")\n",
        "        return {\"complexity\": -1, \"mi_score\": -1, \"loc\": -1}\n",
        "\n",
        "print(\"‚úÖ Core functions defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHmbEXu_38Wx",
        "outputId": "3d007c5c-deaa-41a4-e95c-54e05112f712"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Core functions defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualization Function üìä"
      ],
      "metadata": {
        "id": "jAjdISLZBlb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. Visualization Function üìä\n",
        "# =============================================================================\n",
        "# This function creates bar charts to compare model performance.\n",
        "\n",
        "def plot_metrics(results):\n",
        "    \"\"\"\n",
        "    Generates and displays three bar plots for the collected metrics.\n",
        "    \"\"\"\n",
        "    model_names = list(results.keys())\n",
        "    model_names = [name for name in model_names if results[name]['metrics']['complexity'] != -1]\n",
        "    if not model_names:\n",
        "        print(\"No valid results to plot.\")\n",
        "        return\n",
        "\n",
        "    complexity_scores = [results[name]['metrics']['complexity'] for name in model_names]\n",
        "    mi_scores = [results[name]['metrics']['mi_score'] for name in model_names]\n",
        "    loc_scores = [results[name]['metrics']['loc'] for name in model_names]\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    fig.suptitle('Code Generation Model Performance Comparison', fontsize=16)\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    ax1.bar(model_names, complexity_scores, color=colors)\n",
        "    ax1.set_title('Cyclomatic Complexity (Lower is Better)')\n",
        "    ax1.set_ylabel('Complexity Score')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    ax2.bar(model_names, mi_scores, color=colors)\n",
        "    ax2.set_title('Maintainability Index (Higher is Better)')\n",
        "    ax2.set_ylabel('MI Score')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.set_ylim(0, 100)\n",
        "\n",
        "    ax3.bar(model_names, loc_scores, color=colors)\n",
        "    ax3.set_title('Logical Lines of Code (LOC)')\n",
        "    ax3.set_ylabel('Number of Lines')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization function defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwDlmp4U39o6",
        "outputId": "b21ddfc1-7336-481b-d6bf-b2cf4ec6c8b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Visualization function defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sample Prompts for Testing üß™"
      ],
      "metadata": {
        "id": "mXfHLBSZBnvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. Sample Prompts for Testing üß™\n",
        "# =============================================================================\n",
        "# A list of diverse prompts to test the models.\n",
        "\n",
        "SAMPLE_PROMPTS = [\n",
        "    # Python / Data Science\n",
        "    \"Write a Python function that takes a pandas DataFrame and returns the correlation matrix for its numeric columns.\",\n",
        "    \"Generate a Python script to scrape the headlines from the BBC News homepage using BeautifulSoup.\",\n",
        "    \"Create a Python function to implement the bubble sort algorithm.\",\n",
        "    # Web Development\n",
        "    \"Write a simple HTML page with a form that has fields for 'name', 'email' and a 'submit' button.\",\n",
        "    \"Generate a JavaScript function that fetches data from an API endpoint and logs the JSON response to the console.\",\n",
        "    # Database\n",
        "    \"Write a SQL query to find all employees who earn more than the average salary of their respective departments.\",\n",
        "    \"Write a SQL query to select the top 5 most sold products from a 'sales' table.\",\n",
        "    # General Purpose / DevOps\n",
        "    \"Create a Python function that lists all files in a directory and its subdirectories.\",\n",
        "    \"Write a simple Dockerfile for a basic Python Flask application.\",\n",
        "    # Algorithmic\n",
        "    \"Write a Python function to find the factorial of a number using recursion.\"\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ {len(SAMPLE_PROMPTS)} sample prompts are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bvc_xAy4A6t",
        "outputId": "aab84b27-b511-4571-9b94-067f84ef1758"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 10 sample prompts are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Interactive UI #1: Benchmark All Models ‚ö°Ô∏è"
      ],
      "metadata": {
        "id": "8M40ocKBBp5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. Interactive UI #1: Benchmark All Models ‚ö°Ô∏è\n",
        "# =============================================================================\n",
        "# This UI benchmarks all 5 models for a given prompt.\n",
        "\n",
        "# --- UI Widgets ---\n",
        "prompt_input_1 = widgets.Dropdown(\n",
        "    options=SAMPLE_PROMPTS,\n",
        "    description='Prompt:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout={'width': '95%'}\n",
        ")\n",
        "custom_prompt_input_1 = widgets.Textarea(\n",
        "    placeholder='Or type your own custom prompt here...',\n",
        "    layout={'width': '95%', 'height': '80px'}\n",
        ")\n",
        "benchmark_button = widgets.Button(description=\"Benchmark All Models\", button_style='success')\n",
        "output_1 = widgets.Output()\n",
        "\n",
        "# --- Event Handler ---\n",
        "def on_benchmark_button_clicked(b):\n",
        "    with output_1:\n",
        "        clear_output(wait=True)\n",
        "        prompt = custom_prompt_input_1.value if custom_prompt_input_1.value else prompt_input_1.value\n",
        "        if not prompt:\n",
        "            print(\"üö® Please select or enter a prompt.\")\n",
        "            return\n",
        "\n",
        "        print(f\"üöÄ Starting benchmark for prompt: '{prompt}'\\n\" + \"=\"*52)\n",
        "        results = {}\n",
        "        for model_name in MODELS.keys():\n",
        "            print(f\"--- Processing Model: {model_name} ---\")\n",
        "            model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "            if model is None: continue\n",
        "\n",
        "            generated_code = generate_code(model, tokenizer, prompt)\n",
        "            metrics = evaluate_code(generated_code)\n",
        "            results[model_name] = {\"code\": generated_code, \"metrics\": metrics}\n",
        "\n",
        "            print(f\"\\n‚ú® Generated Code:\\n```python\\n{generated_code}\\n```\")\n",
        "            print(f\"\\nüìä Metrics: {metrics}\")\n",
        "            print(\"\\n\" + \"-\"*52 + \"\\n\")\n",
        "\n",
        "        print(\"üìà Generating performance plots...\")\n",
        "        plot_metrics(results)\n",
        "\n",
        "benchmark_button.on_click(on_benchmark_button_clicked)\n",
        "\n",
        "# --- Display UI ---\n",
        "print(\"UI #1 is ready. Choose a prompt and click the button to start.\")\n",
        "display(prompt_input_1, custom_prompt_input_1, benchmark_button, output_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p7ibTLdb4DbD",
        "outputId": "c89df4df-5d4f-4701-acb8-e0055c9beb72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting benchmark for prompt: 'Create a Python function to implement the bubble sort algorithm.'\n",
            "====================================================\n",
            "--- Processing Model: DeepSeek-Coder-1.3B ---\n",
            "üß† Loading DeepSeek-Coder-1.3B from cache...\n",
            "Generating code for prompt: 'Create a Python function to im...'\n",
            "\n",
            "‚ú® Generated Code:\n",
            "```python\n",
            "def bubble_sort(arr):\n",
            "    n = len(arr)\n",
            "\n",
            "    for i in range(n):\n",
            "        for j in range(0, n - i - 1):\n",
            "            if arr[j] > arr[j + 1]:\n",
            "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
            "\n",
            "    return arr\n",
            "```\n",
            "\n",
            "üìä Metrics: {'complexity': 4, 'mi_score': 68.59, 'loc': 7}\n",
            "\n",
            "----------------------------------------------------\n",
            "\n",
            "--- Processing Model: Phi-2-2.7B ---\n",
            "üß† Loading Phi-2-2.7B from cache...\n",
            "Generating code for prompt: 'Create a Python function to im...'\n",
            "\n",
            "‚ú® Generated Code:\n",
            "```python\n",
            "def bubble_sort(arr):\n",
            "    n = len(arr)\n",
            "    for i in range(n):\n",
            "        for j in range(0, n-i-1):\n",
            "            if arr[j] > arr[j+1]:\n",
            "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
            "    return arr\n",
            "```\n",
            "\n",
            "üìä Metrics: {'complexity': 4, 'mi_score': 68.59, 'loc': 7}\n",
            "\n",
            "----------------------------------------------------\n",
            "\n",
            "--- Processing Model: Gemma-2B-IT ---\n",
            "Downloading and loading Gemma-2B-IT...\n",
            "‚ùå Error loading Gemma-2B-IT: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
            "401 Client Error. (Request ID: Root=1-68ea212d-14b086e12ebac1e75623e644;75fcab4d-bcda-456b-81be-6bbe5e1debae)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
            "--- Processing Model: Stable-Code-3B ---\n",
            "üß† Loading Stable-Code-3B from cache...\n",
            "Generating code for prompt: 'Create a Python function to im...'\n",
            "‚ö†Ô∏è Radon analysis failed: expected ':' (<unknown>, line 70)\n",
            "\n",
            "‚ú® Generated Code:\n",
            "```python\n",
            "def bubble_sort(arr):\n",
            "    \"\"\"\n",
            "    Bubble sort algorithm.\n",
            "    \"\"\"\n",
            "    for i in range(len(arr)):\n",
            "        for j in range(len(arr) - 1):\n",
            "            if arr[j] > arr[j + 1]:\n",
            "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
            "    return arr\n",
            "\n",
            "# Create a Python function to implement the selection sort algorithm.\n",
            "\n",
            "def selection_sort(arr):\n",
            "    \"\"\"\n",
            "    Selection sort algorithm.\n",
            "    \"\"\"\n",
            "    for i in range(len(arr)):\n",
            "        min_index = i\n",
            "        for j in range(i + 1, len(arr)):\n",
            "            if arr[j] < arr[min_index]:\n",
            "                min_index = j\n",
            "        arr[i], arr[min_index] = arr[min_index], arr[i]\n",
            "    return arr\n",
            "\n",
            "# Create a Python function to implement the insertion sort algorithm.\n",
            "\n",
            "def insertion_sort(arr):\n",
            "    \"\"\"\n",
            "    Insertion sort algorithm.\n",
            "    \"\"\"\n",
            "    for i in range(1, len(arr)):\n",
            "        key = arr[i]\n",
            "        j = i - 1\n",
            "        while j >= 0 and key < arr[j]:\n",
            "            arr[j + 1] = arr[j]\n",
            "            j -= 1\n",
            "        arr[j + 1] = key\n",
            "    return arr\n",
            "\n",
            "# Create a Python function to implement the merge sort algorithm.\n",
            "\n",
            "def merge_sort(arr):\n",
            "    \"\"\"\n",
            "    Merge sort algorithm.\n",
            "    \"\"\"\n",
            "    if len(arr) > 1:\n",
            "        mid = len(arr) // 2\n",
            "        left = arr[:mid]\n",
            "        right = arr[mid:]\n",
            "\n",
            "        merge_sort(left)\n",
            "        merge_sort(right)\n",
            "\n",
            "        i = j = k = 0\n",
            "\n",
            "        while i < len(left) and j < len(right):\n",
            "            if left[i] < right[j]:\n",
            "                arr[k] = left[i]\n",
            "                i += 1\n",
            "            else:\n",
            "                arr[k] = right[j]\n",
            "                j += 1\n",
            "            k += 1\n",
            "\n",
            "        while i < len(left):\n",
            "            arr[k] = left[i]\n",
            "            i += 1\n",
            "            k += 1\n",
            "\n",
            "        while j\n",
            "```\n",
            "\n",
            "üìä Metrics: {'complexity': -1, 'mi_score': -1, 'loc': -1}\n",
            "\n",
            "----------------------------------------------------\n",
            "\n",
            "--- Processing Model: Replit-Code-3B ---\n",
            "Downloading and loading Replit-Code-3B...\n",
            "‚ùå Error loading Replit-Code-3B: ReplitLMTokenizer has no attribute vocab_size\n",
            "üìà Generating performance plots...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x600 with 3 Axes>"
            ],
            "image/png": "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