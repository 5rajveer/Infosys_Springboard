{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "state": {},
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes radon ipywidgets matplotlib\n",
        "\n",
        "print(\"‚úÖ All required libraries have been installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSnpBp6H3v6f",
        "outputId": "78469698-da0f-4874-caa1-565efe348061"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All required libraries have been installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports and Global Configuration üìö"
      ],
      "metadata": {
        "id": "UVFbGjz9Bf1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. Imports and Global Configuration üìö\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from radon.visitors import ComplexityVisitor\n",
        "from radon.metrics import mi_visit\n",
        "from radon.raw import analyze\n",
        "\n",
        "# Suppress warnings for a cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Hugging Face Token Setup ---\n",
        "# Tries to get the token from an environment variable.\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "if HF_TOKEN:\n",
        "    print(\"‚úÖ Hugging Face token loaded successfully from environment variable.\")\n",
        "else:\n",
        "    print(\"üö® Could not load HF_TOKEN. Please ensure it's set as an environment variable.\")\n",
        "\n",
        "# --- Model Definitions ---\n",
        "# A dictionary mapping a user-friendly name to its Hugging Face model ID.\n",
        "MODELS = {\n",
        "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
        "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "    \"Stable-Code-3B\": \"stabilityai/stable-code-3b\",\n",
        "    \"Replit-Code-3B\": \"replit/replit-code-v1-3b\",\n",
        "}\n",
        "\n",
        "# --- Caching for Loaded Models ---\n",
        "# This dictionary will store loaded models and tokenizers to avoid reloading them.\n",
        "loaded_models_cache = {}\n",
        "\n",
        "print(\"‚úÖ Imports and configurations are complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcXiERwL35JW",
        "outputId": "e68c48fb-ecac-4a1f-a965-f02ef0f0c334"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö® Could not load HF_TOKEN. Please ensure it's set as an environment variable.\n",
            "‚úÖ Imports and configurations are complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Core Functions: Model Loading, Generation, and Evaluation üöÄ"
      ],
      "metadata": {
        "id": "OwYihRY9Bixz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. Core Functions: Model Loading, Generation, and Evaluation üöÄ\n",
        "# =============================================================================\n",
        "\n",
        "# --- Function to Load Model and Tokenizer ---\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    \"\"\"\n",
        "    Loads a model and tokenizer from Hugging Face with caching.\n",
        "    Uses bfloat16 for efficiency and device_map='auto' for GPU utilization.\n",
        "    \"\"\"\n",
        "    model_id = MODELS[model_name]\n",
        "    if model_id in loaded_models_cache:\n",
        "        print(f\"üß† Loading {model_name} from cache...\")\n",
        "        return loaded_models_cache[model_id]\n",
        "\n",
        "    print(f\"Downloading and loading {model_name}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)\n",
        "        # For Phi-2, pad_token is not set by default. We set it to eos_token.\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            token=HF_TOKEN,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        loaded_models_cache[model_id] = (model, tokenizer)\n",
        "        print(f\"‚úÖ {model_name} loaded successfully.\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# --- Function to Generate Code ---\n",
        "def generate_code(model, tokenizer, prompt):\n",
        "    \"\"\"\n",
        "    Generates code from a given prompt using the specified model and tokenizer.\n",
        "    \"\"\"\n",
        "    print(f\"Generating code for prompt: '{prompt[:30]}...'\")\n",
        "    if \"gemma\" in tokenizer.name_or_path or \"deepseek\" in tokenizer.name_or_path:\n",
        "         messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "         input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        input_text = prompt\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    code_match = re.search(r\"```python\\n(.*?)\\n```\", generated_text, re.DOTALL)\n",
        "    if code_match:\n",
        "        return code_match.group(1).strip()\n",
        "    code_match = re.search(r\"```(.*?)```\", generated_text, re.DOTALL)\n",
        "    if code_match:\n",
        "        return code_match.group(1).strip().lstrip('python\\n')\n",
        "    return generated_text[len(input_text):].strip()\n",
        "\n",
        "# --- Function to Evaluate Code Quality ---\n",
        "def evaluate_code(code_string):\n",
        "    \"\"\"\n",
        "    Analyzes a string of Python code using 'radon' and returns quality metrics.\n",
        "    \"\"\"\n",
        "    if not code_string:\n",
        "        return {\"complexity\": 0, \"mi_score\": 0, \"loc\": 0}\n",
        "    try:\n",
        "        visitor = ComplexityVisitor.from_code(code_string)\n",
        "        total_complexity = sum(f.complexity for f in visitor.functions)\n",
        "        mi_score = mi_visit(code_string, multi=True)\n",
        "        raw_analysis = analyze(code_string)\n",
        "        loc = raw_analysis.lloc\n",
        "        return {\n",
        "            \"complexity\": total_complexity,\n",
        "            \"mi_score\": round(mi_score, 2),\n",
        "            \"loc\": loc\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Radon analysis failed: {e}\")\n",
        "        return {\"complexity\": -1, \"mi_score\": -1, \"loc\": -1}\n",
        "\n",
        "print(\"‚úÖ Core functions defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHmbEXu_38Wx",
        "outputId": "3d007c5c-deaa-41a4-e95c-54e05112f712"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Core functions defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualization Function üìä"
      ],
      "metadata": {
        "id": "jAjdISLZBlb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. Visualization Function üìä\n",
        "# =============================================================================\n",
        "# This function creates bar charts to compare model performance.\n",
        "\n",
        "def plot_metrics(results):\n",
        "    \"\"\"\n",
        "    Generates and displays three bar plots for the collected metrics.\n",
        "    \"\"\"\n",
        "    model_names = list(results.keys())\n",
        "    model_names = [name for name in model_names if results[name]['metrics']['complexity'] != -1]\n",
        "    if not model_names:\n",
        "        print(\"No valid results to plot.\")\n",
        "        return\n",
        "\n",
        "    complexity_scores = [results[name]['metrics']['complexity'] for name in model_names]\n",
        "    mi_scores = [results[name]['metrics']['mi_score'] for name in model_names]\n",
        "    loc_scores = [results[name]['metrics']['loc'] for name in model_names]\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    fig.suptitle('Code Generation Model Performance Comparison', fontsize=16)\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    ax1.bar(model_names, complexity_scores, color=colors)\n",
        "    ax1.set_title('Cyclomatic Complexity (Lower is Better)')\n",
        "    ax1.set_ylabel('Complexity Score')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    ax2.bar(model_names, mi_scores, color=colors)\n",
        "    ax2.set_title('Maintainability Index (Higher is Better)')\n",
        "    ax2.set_ylabel('MI Score')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.set_ylim(0, 100)\n",
        "\n",
        "    ax3.bar(model_names, loc_scores, color=colors)\n",
        "    ax3.set_title('Logical Lines of Code (LOC)')\n",
        "    ax3.set_ylabel('Number of Lines')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization function defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwDlmp4U39o6",
        "outputId": "b21ddfc1-7336-481b-d6bf-b2cf4ec6c8b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Visualization function defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sample Prompts for Testing üß™"
      ],
      "metadata": {
        "id": "mXfHLBSZBnvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. Sample Prompts for Testing üß™\n",
        "# =============================================================================\n",
        "# A list of diverse prompts to test the models.\n",
        "\n",
        "SAMPLE_PROMPTS = [\n",
        "    # Python / Data Science\n",
        "    \"Write a Python function that takes a pandas DataFrame and returns the correlation matrix for its numeric columns.\",\n",
        "    \"Generate a Python script to scrape the headlines from the BBC News homepage using BeautifulSoup.\",\n",
        "    \"Create a Python function to implement the bubble sort algorithm.\",\n",
        "    # Web Development\n",
        "    \"Write a simple HTML page with a form that has fields for 'name', 'email' and a 'submit' button.\",\n",
        "    \"Generate a JavaScript function that fetches data from an API endpoint and logs the JSON response to the console.\",\n",
        "    # Database\n",
        "    \"Write a SQL query to find all employees who earn more than the average salary of their respective departments.\",\n",
        "    \"Write a SQL query to select the top 5 most sold products from a 'sales' table.\",\n",
        "    # General Purpose / DevOps\n",
        "    \"Create a Python function that lists all files in a directory and its subdirectories.\",\n",
        "    \"Write a simple Dockerfile for a basic Python Flask application.\",\n",
        "    # Algorithmic\n",
        "    \"Write a Python function to find the factorial of a number using recursion.\"\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ {len(SAMPLE_PROMPTS)} sample prompts are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bvc_xAy4A6t",
        "outputId": "aab84b27-b511-4571-9b94-067f84ef1758"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 10 sample prompts are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Interactive UI #1: Benchmark All Models ‚ö°Ô∏è"
      ],
      "metadata": {
        "id": "8M40ocKBBp5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. Interactive UI #1: Benchmark All Models ‚ö°Ô∏è\n",
        "# =============================================================================\n",
        "# This UI benchmarks all 5 models for a given prompt.\n",
        "\n",
        "# --- UI Widgets ---\n",
        "prompt_input_1 = widgets.Dropdown(\n",
        "    options=SAMPLE_PROMPTS,\n",
        "    description='Prompt:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout={'width': '95%'}\n",
        ")\n",
        "custom_prompt_input_1 = widgets.Textarea(\n",
        "    placeholder='Or type your own custom prompt here...',\n",
        "    layout={'width': '95%', 'height': '80px'}\n",
        ")\n",
        "benchmark_button = widgets.Button(description=\"Benchmark All Models\", button_style='success')\n",
        "output_1 = widgets.Output()\n",
        "\n",
        "# --- Event Handler ---\n",
        "def on_benchmark_button_clicked(b):\n",
        "    with output_1:\n",
        "        clear_output(wait=True)\n",
        "        prompt = custom_prompt_input_1.value if custom_prompt_input_1.value else prompt_input_1.value\n",
        "        if not prompt:\n",
        "            print(\"üö® Please select or enter a prompt.\")\n",
        "            return\n",
        "\n",
        "        print(f\"üöÄ Starting benchmark for prompt: '{prompt}'\\n\" + \"=\"*52)\n",
        "        results = {}\n",
        "        for model_name in MODELS.keys():\n",
        "            print(f\"--- Processing Model: {model_name} ---\")\n",
        "            model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "            if model is None: continue\n",
        "\n",
        "            generated_code = generate_code(model, tokenizer, prompt)\n",
        "            metrics = evaluate_code(generated_code)\n",
        "            results[model_name] = {\"code\": generated_code, \"metrics\": metrics}\n",
        "\n",
        "            print(f\"\\n‚ú® Generated Code:\\n```python\\n{generated_code}\\n```\")\n",
        "            print(f\"\\nüìä Metrics: {metrics}\")\n",
        "            print(\"\\n\" + \"-\"*52 + \"\\n\")\n",
        "\n",
        "        print(\"üìà Generating performance plots...\")\n",
        "        plot_metrics(results)\n",
        "\n",
        "benchmark_button.on_click(on_benchmark_button_clicked)\n",
        "\n",
        "# --- Display UI ---\n",
        "print(\"UI #1 is ready. Choose a prompt and click the button to start.\")\n",
        "display(prompt_input_1, custom_prompt_input_1, benchmark_button, output_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p7ibTLdb4DbD",
        "outputId": "c89df4df-5d4f-4701-acb8-e0055c9beb72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UI #1 is ready. Choose a prompt and click the button to start.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "549f6f827d854d9ba02f622b9c1198f1"
            },
            "text/plain": [
              "Dropdown(description='Prompt:', index=2, layout=Layout(width='95%'), options=('Write a Python function that t‚Ä¶"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "850e5f9fee234f2eabfde40bb24a85a5"
            },
            "text/plain": [
              "Textarea(value='', layout=Layout(height='80px', width='95%'), placeholder='Or type your own custom prompt h‚Ä¶"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7298f81920624dfaa1620e4b87ca03ca"
            },
            "text/plain": [
              "Button(button_style='success', description='Benchmark All Models', style=ButtonStyle())"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b2fce7187b546d78c45ac473e4e24ef"
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Interactive UI #2: Inspect Models with Checkboxes ‚úÖ"
      ],
      "metadata": {
        "id": "nfMSUFyABs1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 7. Interactive UI #2: Inspect Models with Checkboxes ‚úÖ\n",
        "# =============================================================================\n",
        "# This UI allows you to select specific models to compare.\n",
        "\n",
        "# --- UI Widgets ---\n",
        "prompt_input_2 = widgets.Dropdown(\n",
        "    options=SAMPLE_PROMPTS,\n",
        "    description='Prompt:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout={'width': '95%'}\n",
        ")\n",
        "custom_prompt_input_2 = widgets.Textarea(\n",
        "    placeholder='Or type your own custom prompt here...',\n",
        "    layout={'width': '95%', 'height': '80px'}\n",
        ")\n",
        "model_checkboxes = [widgets.Checkbox(value=True, description=name) for name in MODELS.keys()]\n",
        "checkbox_container = widgets.HBox(model_checkboxes)\n",
        "inspect_button = widgets.Button(description=\"Generate & Inspect Selected\", button_style='primary')\n",
        "output_2 = widgets.Output()\n",
        "\n",
        "# --- Event Handler ---\n",
        "def on_inspect_button_clicked(b):\n",
        "    with output_2:\n",
        "        clear_output(wait=True)\n",
        "        prompt = custom_prompt_input_2.value if custom_prompt_input_2.value else prompt_input_2.value\n",
        "        if not prompt:\n",
        "            print(\"üö® Please select or enter a prompt.\")\n",
        "            return\n",
        "\n",
        "        selected_models = [cb.description for cb in model_checkboxes if cb.value]\n",
        "        if not selected_models:\n",
        "            print(\"üö® Please select at least one model to inspect.\")\n",
        "            return\n",
        "\n",
        "        print(f\"üöÄ Starting inspection for prompt: '{prompt}'\")\n",
        "        print(f\"Models selected: {', '.join(selected_models)}\\n\" + \"=\"*52)\n",
        "        results = {}\n",
        "        for model_name in selected_models:\n",
        "            print(f\"--- Processing Model: {model_name} ---\")\n",
        "            model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "            if model is None: continue\n",
        "\n",
        "            generated_code = generate_code(model, tokenizer, prompt)\n",
        "            metrics = evaluate_code(generated_code)\n",
        "            results[model_name] = {\"code\": generated_code, \"metrics\": metrics}\n",
        "\n",
        "            print(f\"\\n‚ú® Generated Code:\\n```python\\n{generated_code}\\n```\")\n",
        "            print(f\"\\nüìä Metrics: {metrics}\")\n",
        "            print(\"\\n\" + \"-\"*52 + \"\\n\")\n",
        "\n",
        "        print(\"üìà Generating performance plots...\")\n",
        "        plot_metrics(results)\n",
        "\n",
        "inspect_button.on_click(on_inspect_button_clicked)\n",
        "\n",
        "# --- Display UI ---\n",
        "print(\"\\n\\nUI #2 is ready. Choose a prompt, select models, and click the button.\")\n",
        "display(prompt_input_2, custom_prompt_input_2, checkbox_container, inspect_button, output_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Wx9gV9o4GD0",
        "outputId": "0f44df41-84fe-4603-8a81-08a88b39a023"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "UI #2 is ready. Choose a prompt, select models, and click the button.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a930dd211e647d299a8c2093788b927"
            },
            "text/plain": [
              "Dropdown(description='Prompt:', index=2, layout=Layout(width='95%'), options=('Write a Python function that t‚Ä¶"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9bf704fc08048ecb5bd4386f1e8ce2a"
            },
            "text/plain": [
              "Textarea(value='', layout=Layout(height='80px', width='95%'), placeholder='Or type your own custom prompt h‚Ä¶"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0339034f798242c59b4fd91470c0f15c"
            },
            "text/plain": [
              "HBox(children=(Checkbox(value=True, description='DeepSeek-Coder-1.3B'), Checkbox(value=True, description='Phi‚Ä¶"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9c8be7cec974197995b7beea8a433fb"
            },
            "text/plain": [
              "Button(button_style='primary', description='Generate & Inspect Selected', style=ButtonStyle())"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cce96b5c91a4af9bbb7ab856b28e23d"
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}